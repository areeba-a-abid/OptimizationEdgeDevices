{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIH_14_DATASET_PATH = './NIH_14/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.abspath(NIH_14_DATASET_PATH) \n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entry_csv_path = os.path.join(dataset_path, 'Data_Entry_2017.csv')\n",
    "data = pd.read_csv(data_entry_csv_path)\n",
    "print(f\"Data Shape : {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Patient Age']<100]\n",
    "\n",
    "print(f\"New dataset dimensions: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Image Index', 'Finding Labels']]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = sorted(glob2.glob(dataset_path + '/**/*.png'))\n",
    "print(f'Number of Images: {len(all_images)}')\n",
    "\n",
    "all_image_paths = {os.path.basename(x): x for x in all_images}\n",
    "\n",
    "#Add path of images as column to the dataset\n",
    "data['Path'] = data['Image Index'].map(all_image_paths.get)\n",
    "data.sample(5, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*data['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.delete(all_labels, np.where(all_labels == 'No Finding'))\n",
    "all_labels = [x for x in all_labels]\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        # Add a column for each desease\n",
    "        data[c_label] = data['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n",
    "        \n",
    "print(f\"Dataset Dimension: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = data['Finding Labels'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('Finding Labels').filter(lambda x : len(x)>11)\n",
    "label_counts = data['Finding Labels'].value_counts()\n",
    "print(label_counts.shape)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_valid_df, test_df = train_test_split(data,\n",
    "                                               test_size = 0.30,\n",
    "                                               random_state = 2018,\n",
    "                                              )\n",
    "\n",
    "train_df, valid_df = train_test_split(train_and_valid_df,\n",
    "                                      test_size=0.30,\n",
    "                                      random_state=2018,\n",
    "                                     )\n",
    "\n",
    "print(f'Training: {train_df.shape[0]} Validation: {valid_df.shape[0]} Testing: {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "def flow_from_dataframe(image_generator, dataframe, batch_size):\n",
    "\n",
    "    df_gen = image_generator.flow_from_dataframe(dataframe,\n",
    "                                                 x_col='Path',\n",
    "                                                 y_col=all_labels,\n",
    "                                                 target_size=IMG_SIZE,\n",
    "                                                 classes=all_labels,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='raw',\n",
    "                                                 shuffle=False,\n",
    "                                                 batch_size=batch_size)\n",
    "    \n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                                dataframe= train_df,\n",
    "                                batch_size = 32)\n",
    "\n",
    "valid_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                                dataframe=valid_df,\n",
    "                                batch_size = 32)\n",
    "\n",
    "test_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                               dataframe=test_df,\n",
    "                               batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = next(train_gen)\n",
    "print(f\"Image Dimensions: {train_x[1].shape}\")\n",
    "print(f\"Labels: {train_y[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape=(224, 224, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "base_model = DenseNet121(include_top=False, input_tensor=img_input, input_shape=input_shape, \n",
    "                         pooling=\"avg\", weights='imagenet')\n",
    "x = base_model.output\n",
    "\n",
    "predictions = Dense(len(all_labels), activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "model = Model(inputs=img_input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_helper import EvaluationHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance = EvaluationHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_FP32_MODEL_PATH = os.path.abspath('./weights/baseline_FP32.h5')\n",
    "BASELINE_QAT_FP32_MODEL_PATH = os.path.abspath('./weights/baseline_QAT_FP32.h5')\n",
    "INT8_TFLITE_MODEL_PATH = os.path.abspath('./weights/QAT_INT8.tflite')\n",
    "FP16_TFLITE_MODEL_PATH = os.path.abspath('./weights/FP16.tflite')\n",
    "DYNAMIC_QUANTIZED_TFLITE_MODEL_PATH = os.path.abspath('./weights/dynamic_quantized.tflite')\n",
    "FP32_NO_QUANTIZED_TFLITE_MODEL_PATH = os.path.abspath('./weights/FP32_no_quantization.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.test_generator = test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(BASELINE_FP32_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred = evaluation_helper_instance.get_model_predictions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = evaluation_helper_instance.get_auc_roc_score(baseline_pred)\n",
    "print(f\"Baseline FP32 AUC Score : {auc_score}\")\n",
    "with open('baseline_FP32_auc_score.txt', 'w') as fp:\n",
    "    fp.write(f\"AUC-ROC score for FP32 baseline model is {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.get_auc_plot(baseline_pred, all_labels, 'baseline_FP32.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.qunantized_model_path = FP16_TFLITE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_16_pred = evaluation_helper_instance.get_tflite_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = evaluation_helper_instance.get_auc_roc_score(fp_16_pred)\n",
    "print(f\"FP16 AUC Score : {auc_score}\")\n",
    "with open('FP16_auc_score.txt', 'w') as fp:\n",
    "    fp.write(f\"AUC-ROC score for FP16 Quantized model is {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.get_auc_plot(fp_16_pred, all_labels, 'FP16.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.qunantized_model_path = DYNAMIC_QUANTIZED_TFLITE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_quant_pred = evaluation_helper_instance.get_tflite_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = evaluation_helper_instance.get_auc_roc_score(dynamic_quant_pred)\n",
    "print(f\"Dynamic Quant AUC Score : {auc_score}\")\n",
    "with open('DynamicQuant_auc_score.txt', 'w') as fp:\n",
    "    fp.write(f\"AUC-ROC score for Dynamic Quantized model is {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.get_auc_plot(dynamic_quant_pred, all_labels, 'DynamicQunat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.qunantized_model_path = INT8_TFLITE_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_quant_pred = evaluation_helper_instance.get_tflite_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = evaluation_helper_instance.get_auc_roc_score(int8_quant_pred)\n",
    "print(f\"INT8 Quant AUC Score : {auc_score}\")\n",
    "with open('INT8_auc_score.txt', 'w') as fp:\n",
    "    fp.write(f\"AUC-ROC score for INT8 Quantized model is {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.get_auc_plot(int8_quant_pred, all_labels, 'INT8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "    \n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "    \n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(\n",
    "    num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization_to_batch_normalization(layer):\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        return quantize_annotate_layer(layer, DefaultBNQuantizeConfig())\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "                    model,\n",
    "                    clone_function=apply_quantization_to_batch_normalization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with quantize_scope(\n",
    "  {'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model.load_weights(BASELINE_QAT_FP32_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_baseline_FP32_pred = evaluation_helper_instance.get_model_predictions(quant_aware_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = evaluation_helper_instance.get_auc_roc_score(qat_baseline_FP32_pred)\n",
    "print(f\"QAT Baseline FP32 AUC Score : {auc_score}\")\n",
    "with open('QAT_baseline_FP32_auc_score.txt', 'w') as fp:\n",
    "    fp.write(f\"AUC-ROC score for QAT FP32 baseline model is {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_helper_instance.get_auc_plot(qat_baseline_FP32_pred, all_labels, 'QAT_baseline_FP32.png')"
   ]
  }
 ]
}